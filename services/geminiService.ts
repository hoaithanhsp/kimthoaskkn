
import { GoogleGenAI, Chat } from "@google/genai";
import { SYSTEM_INSTRUCTION, FALLBACK_MODELS } from "../constants";

// H√†m ph√¢n t√≠ch v√† tr·∫£ v·ªÅ th√¥ng b√°o l·ªói th√¢n thi·ªán
const parseApiError = (error: any): string => {
  const errorMessage = error?.message || error?.toString() || '';
  const errorString = JSON.stringify(error);

  // Ki·ªÉm tra l·ªói quota exceeded (429)
  if (errorString.includes('429') ||
    errorMessage.includes('quota') ||
    errorMessage.includes('RESOURCE_EXHAUSTED') ||
    errorMessage.includes('exceeded')) {
    return 'QUOTA_EXCEEDED';
  }

  // Ki·ªÉm tra l·ªói rate limit
  if (errorMessage.includes('rate') || errorMessage.includes('limit')) {
    return 'RATE_LIMIT';
  }

  // Ki·ªÉm tra l·ªói API key kh√¥ng h·ª£p l·ªá
  if (errorMessage.includes('API_KEY_INVALID') ||
    errorMessage.includes('401') ||
    errorMessage.includes('unauthorized') ||
    errorMessage.includes('PERMISSION_DENIED')) {
    return 'INVALID_API_KEY';
  }

  // Ki·ªÉm tra l·ªói k·∫øt n·ªëi
  if (errorMessage.includes('network') ||
    errorMessage.includes('fetch') ||
    errorMessage.includes('connection')) {
    return 'NETWORK_ERROR';
  }

  return 'UNKNOWN';
};

// H√†m t·∫°o th√¥ng b√°o l·ªói th√¢n thi·ªán
export const getFriendlyErrorMessage = (error: any): { type: string; title: string; message: string; suggestions: string[] } => {
  const errorType = parseApiError(error);

  switch (errorType) {
    case 'QUOTA_EXCEEDED':
      return {
        type: 'quota',
        title: '‚ö†Ô∏è ƒê√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n s·ª≠ d·ª•ng',
        message: 'B·∫°n ƒë√£ s·ª≠ d·ª•ng h·∫øt l∆∞·ª£t g·ªçi API mi·ªÖn ph√≠ trong ng√†y. ƒê√¢y l√† gi·ªõi h·∫°n t·ª´ ph√≠a Google, kh√¥ng ph·∫£i l·ªói c·ªßa ·ª©ng d·ª•ng.',
        suggestions: [
          '‚è∞ ƒê·ª£i kho·∫£ng 1-2 ph√∫t r·ªìi th·ª≠ l·∫°i',
          'üîë S·ª≠ d·ª•ng API Key kh√°c n·∫øu c√≥',
          'üìÖ ƒê·ª£i ƒë·∫øn ng√†y h√¥m sau khi quota ƒë∆∞·ª£c reset',
          'üí≥ N√¢ng c·∫•p t√†i kho·∫£n Google AI Studio ƒë·ªÉ c√≥ th√™m quota'
        ]
      };

    case 'RATE_LIMIT':
      return {
        type: 'rate_limit',
        title: 'üö¶ ƒêang g·ª≠i y√™u c·∫ßu qu√° nhanh',
        message: 'B·∫°n ƒëang g·ª≠i qu√° nhi·ªÅu y√™u c·∫ßu trong th·ªùi gian ng·∫Øn. H√£y ch·ªù m·ªôt ch√∫t r·ªìi th·ª≠ l·∫°i.',
        suggestions: [
          '‚è≥ ƒê·ª£i 30-60 gi√¢y r·ªìi th·ª≠ l·∫°i',
          'üîÑ Kh√¥ng b·∫•m n√∫t nhi·ªÅu l·∫ßn li√™n ti·∫øp'
        ]
      };

    case 'INVALID_API_KEY':
      return {
        type: 'auth',
        title: 'üîê API Key kh√¥ng h·ª£p l·ªá',
        message: 'API Key b·∫°n ƒëang s·ª≠ d·ª•ng kh√¥ng ƒë√∫ng ho·∫∑c ƒë√£ h·∫øt h·∫°n.',
        suggestions: [
          'üîë Ki·ªÉm tra l·∫°i API Key ƒë√£ nh·∫≠p',
          'üÜï T·∫°o API Key m·ªõi t·∫°i Google AI Studio',
          'üìã ƒê·∫£m b·∫£o copy ƒë·∫ßy ƒë·ªß API Key (kh√¥ng th·ª´a/thi·∫øu k√Ω t·ª±)'
        ]
      };

    case 'NETWORK_ERROR':
      return {
        type: 'network',
        title: 'üåê L·ªói k·∫øt n·ªëi m·∫°ng',
        message: 'Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn m√°y ch·ªß Google AI. H√£y ki·ªÉm tra k·∫øt n·ªëi internet c·ªßa b·∫°n.',
        suggestions: [
          'üì∂ Ki·ªÉm tra k·∫øt n·ªëi WiFi/Internet',
          'üîÑ Th·ª≠ l√†m m·ªõi trang (F5)',
          'üåç Th·ª≠ s·ª≠ d·ª•ng m·∫°ng kh√°c'
        ]
      };

    default:
      return {
        type: 'unknown',
        title: '‚ùå ƒê√£ x·∫£y ra l·ªói',
        message: error?.message || 'C√≥ l·ªói kh√¥ng x√°c ƒë·ªãnh x·∫£y ra khi g·ªçi AI.',
        suggestions: [
          'üîÑ Th·ª≠ l√†m m·ªõi trang v√† th·ª±c hi·ªán l·∫°i',
          'üîë Ki·ªÉm tra API Key',
          '‚è∞ ƒê·ª£i m·ªôt l√∫c r·ªìi th·ª≠ l·∫°i'
        ]
      };
  }
};

let chatSession: Chat | null = null;
let currentApiKey: string | null = null;
let currentSelectedModel: string | null = null;
let history: any[] = []; // Store history to restore when switching models

export const initializeGeminiChat = (apiKey: string, selectedModel?: string) => {
  currentApiKey = apiKey;
  currentSelectedModel = selectedModel || FALLBACK_MODELS[0];
  chatSession = null;
  history = []; // Reset history on new initialization
};

const createChatSession = (model: string) => {
  if (!currentApiKey) throw new Error("API Key not found");

  const ai = new GoogleGenAI({ apiKey: currentApiKey });

  return ai.chats.create({
    model: model,
    config: {
      systemInstruction: SYSTEM_INSTRUCTION,
      temperature: 0.7,
      topK: 64,
      topP: 0.95,
      maxOutputTokens: 65536,
      thinkingConfig: { thinkingBudget: 2048 },
      tools: [{ googleSearch: {} }]
    },
    history: history
  });
};

// S·∫Øp x·∫øp models v·ªõi model ƒë∆∞·ª£c ch·ªçn ƒë·∫ßu ti√™n
const getOrderedModels = (): string[] => {
  if (!currentSelectedModel || !FALLBACK_MODELS.includes(currentSelectedModel)) {
    return FALLBACK_MODELS;
  }

  // ƒê∆∞a model ƒë∆∞·ª£c ch·ªçn l√™n ƒë·∫ßu, gi·ªØ nguy√™n th·ª© t·ª± c√°c model c√≤n l·∫°i
  const orderedModels = [currentSelectedModel];
  for (const model of FALLBACK_MODELS) {
    if (model !== currentSelectedModel) {
      orderedModels.push(model);
    }
  }
  return orderedModels;
};

export const sendMessageStream = async (message: string, onChunk: (text: string) => void) => {
  if (!currentApiKey) throw new Error("API Key not initialized");

  let lastError: any = null;
  const modelsToTry = getOrderedModels();

  // Try through the fallback models
  for (const model of modelsToTry) {
    try {
      console.log(`ü§ñ ƒêang th·ª≠ model: ${model}`);

      // Always recreate session with current history to ensure we use the selected model
      // (or optimize to reuse if same model, but recreation is safer for fallback)
      chatSession = createChatSession(model);

      const responseStream = await chatSession.sendMessageStream({ message });

      let fullResponse = "";
      for await (const chunk of responseStream) {
        if (chunk.text) {
          onChunk(chunk.text);
          fullResponse += chunk.text;
        }
      }

      // If successful, update history and break
      history.push({ role: 'user', parts: [{ text: message }] });
      history.push({ role: 'model', parts: [{ text: fullResponse }] });
      console.log(`‚úÖ Model ${model} th√†nh c√¥ng!`);
      return;

    } catch (error: any) {
      console.error(`‚ùå Model ${model} th·∫•t b·∫°i:`, error.message || error);
      lastError = error;
      // Continue to next model
    }
  }

  // If all models fail
  throw lastError || new Error("T·∫•t c·∫£ models ƒë·ªÅu th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i sau.");
};
